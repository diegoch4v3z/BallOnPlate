

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>NxSDK Performance Test Suite &mdash; NxSDK 1.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="NxSDK Release History" href="README.html" />
    <link rel="prev" title="Trace Injection Module" href="trace_injection.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> NxSDK
          

          
          </a>

          
            
            
              <div class="version">
                1.0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introductory Material</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="nxapi.html">NxAPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="nxnet.html">NxNet API</a></li>
<li class="toctree-l1"><a class="reference internal" href="nxcore.html">NxCore API</a></li>
<li class="toctree-l1"><a class="reference internal" href="composable.html">Composability</a></li>
<li class="toctree-l1"><a class="reference internal" href="embedded_execution_engine.html">Embedded Execution Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Nx SDK Modules</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">NxSDK Performance Test Suite</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#run-subset-of-performance-tests-given-a-pattern">Run subset of performance tests given a pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="#run-a-single-performance-test">Run a single performance test</a></li>
<li class="toctree-l2"><a class="reference internal" href="#measuring-performance-of-user-scripts">Measuring performance of user scripts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tag-a-performance-measurment-for-reference-history">Tag a performance measurment for reference/history</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compare-performance-runs-historial-comparisons">Compare performance runs (Historial Comparisons)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#add-a-new-performance-test">Add a new performance test</a></li>
<li class="toctree-l2"><a class="reference internal" href="#add-a-custom-metric">Add a custom metric</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="README.html">NxSDK Release History</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NxSDK</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>NxSDK Performance Test Suite</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="nxsdk-performance-test-suite">
<h1>NxSDK Performance Test Suite<a class="headerlink" href="#nxsdk-performance-test-suite" title="Permalink to this headline">¶</a></h1>
<p>NxSDK comes with a performance test suite and associated Python decorators and metrics to create new benchmarks.</p>
<p>Performance tests are packaged within nxsdk-apps.</p>
<p>To run the full performance suite:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">nxsdk-apps</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">performance.benchmark</span></code></p></li>
</ol>
<p>This creates a <strong>benchmarks.html</strong> file which can be opened in any web browser</p>
<hr class="docutils" />
<div class="section" id="run-subset-of-performance-tests-given-a-pattern">
<h2>Run subset of performance tests given a pattern<a class="headerlink" href="#run-subset-of-performance-tests-given-a-pattern" title="Permalink to this headline">¶</a></h2>
<p>To run a <strong>subset</strong> of performance tests given a <strong>pattern</strong>, run:</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">performance.benchmark</span> <span class="pre">-p</span> <span class="pre">*basic*py</span></code></p>
<p>This runs all tests which match the pattern <strong>*basic*py</strong> like test_basic_spike_injection.py</p>
</div>
<hr class="docutils" />
<div class="section" id="run-a-single-performance-test">
<h2>Run a single performance test<a class="headerlink" href="#run-a-single-performance-test" title="Permalink to this headline">¶</a></h2>
<p>Each performance test comprises of a <strong>main</strong> function and can be run individually as:</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">performance.snips.test_basic_spike_injection</span></code></p>
<p>You may run it similarly using an IDE for debugging or adding enhancements. The metrics will be printed onto the console along with the output of the test.</p>
</div>
<hr class="docutils" />
<div class="section" id="measuring-performance-of-user-scripts">
<h2>Measuring performance of user scripts<a class="headerlink" href="#measuring-performance-of-user-scripts" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Users can collect execution and energy statistics on their networks by leveraging the environment variables used by the performance suite to collect the metrics. See below relevant environment variables.</p>
</div>
<ul class="simple">
<li><p><em>NXSDK_ENABLE_STATS=1</em> : Set this to enable collection of all
metrics (Setting it inserts Execution and Energy Probes implicitly
into your network).</p>
<ul>
<li><p>Usage: <code class="docutils literal notranslate"><span class="pre">NXSDK_ENABLE_STATS=1</span> <span class="pre">python</span> <span class="pre">&lt;my_script.py&gt;</span></code></p></li>
</ul>
</li>
<li><p><em>NXSDK_DISABLE_ENERGY_STATS=1</em> : Energy statistics sometimes need
larger networks/longer runs and more sophisticated tuning. So it might
not be relevant for all scenarios. For instance, most of the
workloads in performance suite have it disabled. Note, by default,
NXSDK_ENABLE_STATS=1 inserts both kinds of performance probes.</p>
<ul>
<li><p>Usage: <code class="docutils literal notranslate"><span class="pre">NXSDK_DISABLE_ENERGY_STATS=1</span> <span class="pre">NXSDK_ENABLE_STATS=1</span> <span class="pre">python</span> <span class="pre">&lt;my_script.py&gt;</span></code></p></li>
</ul>
</li>
</ul>
<p>Setting these environment variables will enable the performance logging and spit out all the metrics for all phases of run including compilation, execution and post-processing with energy measurements.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>These metrics should be used only for <strong>debugging</strong> and <strong>optimization</strong> purposes. Competitive benchmarking should avoid these global environment variables which give you only a macro view of the execution and might not be fully accurate. Instead, use the underlying execution and energy probes with correct initialization parameters for accurate measurements. See Performance Profiling jupyter tutorial for examples.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="tag-a-performance-measurment-for-reference-history">
<h2>Tag a performance measurment for reference/history<a class="headerlink" href="#tag-a-performance-measurment-for-reference-history" title="Permalink to this headline">¶</a></h2>
<p>To run a benchmark and <strong>tag</strong> your run, execute:</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">performance.benchmark</span> <span class="pre">-t</span> <span class="pre">run1</span></code></p>
<p>If you want to just run a subset and tag the same: <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">performance.benchmark</span> <span class="pre">-t</span> <span class="pre">run-basic</span> <span class="pre">-p</span> <span class="pre">*basic*py</span></code></p>
<p><em>Tags are strings and only stand for your reference so that you may compare historical runs between software and hardware improvements/upgrades.</em></p>
<p>Each run is saved as a JSON file under a directory <em>.nxsdk-benchmarks</em> with the name <code class="docutils literal notranslate"><span class="pre">tag</span></code>.json</p>
</div>
<hr class="docutils" />
<div class="section" id="compare-performance-runs-historial-comparisons">
<h2>Compare performance runs (Historial Comparisons)<a class="headerlink" href="#compare-performance-runs-historial-comparisons" title="Permalink to this headline">¶</a></h2>
<p>To run an analysis of all your runs, we provide a benchmark comparison tool:</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">performance.benchmark_comparison</span></code></p>
<p>This creates a <strong>benchmarks_history.html</strong> file which can be opened in any web browser.</p>
<p>This runs against the <em>.nxsdk-benchmarks</em> directory and creates a comparison plot of all benchmarks which have run and tagged.</p>
<p>You might also point to a custom location. See <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">performance.benchmark_comparison</span> <span class="pre">--help</span></code> for more options.</p>
</div>
<hr class="docutils" />
<div class="section" id="add-a-new-performance-test">
<h2>Add a new performance test<a class="headerlink" href="#add-a-new-performance-test" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Create a directory structure (to group your tests) or add a file similar to existing benchmarks</p></li>
<li><p>Inherit your class from PerfLoggingHandler</p></li>
<li><p>Use the timeit decorator to add timing logs</p>
<ol class="arabic simple">
<li><p>&#64;timeit can be imported as: <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">nxsdk.logutils.benchmark_utils</span> <span class="pre">import</span> <span class="pre">timeit</span></code></p></li>
<li><p>Then it can be added to any method as a decorator</p></li>
</ol>
</li>
<li><p>Add a main method to make your test executable</p></li>
</ol>
</div>
<hr class="docutils" />
<div class="section" id="add-a-custom-metric">
<h2>Add a custom metric<a class="headerlink" href="#add-a-custom-metric" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>Create a class inheriting from AbstractMetric. For e.g</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyCustomMetric</span><span class="p">(</span><span class="n">AbstractMetric</span><span class="p">,</span> <span class="n">LowerIsBetterMetric</span><span class="p">):</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">AbstractMetric</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;My Metric Label&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="s2">&quot;MyMetricUnit&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Use appropriate labels and units for your metric.</p>
<p>We provide two abstractions: <strong>LowerIsBetterMetric</strong> and <strong>HigherIsBetterMetric</strong>.
Usually time and power related metrics might inherit from LowerIsBetterMetric while model accuracy might be a HigherIsBetterMetric.</p>
</li>
<li><p>Define your performance test by following the <strong>Add a new performance test</strong> mentioned above</p></li>
<li><p>To add your new custom metric, you need to import the &#64;reportit decorator</p>
<ol class="arabic simple">
<li><p>&#64;reportit can be imported as: <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">nxsdk.logutils.benchmark_utils</span> <span class="pre">import</span> <span class="pre">reportit,</span> <span class="pre">timeit</span></code></p></li>
<li><p>Then it can be added to any method as a decorator</p></li>
<li><p>Ensure to return this metric from the function being benchmarked. For e.g., if you are testing method_x</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@timeit</span>
<span class="nd">@reportit</span>
<span class="k">def</span> <span class="nf">time_method_x</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="o">...</span><span class="n">some</span> <span class="n">computation</span> <span class="n">to</span> <span class="n">test</span><span class="o">...</span>
    <span class="k">return</span> <span class="n">MyCustomMetric</span><span class="p">(</span><span class="n">some_value</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Run the performance test</p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="README.html" class="btn btn-neutral float-right" title="NxSDK Release History" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="trace_injection.html" class="btn btn-neutral float-left" title="Trace Injection Module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2018-2021, Intel Corporation.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>